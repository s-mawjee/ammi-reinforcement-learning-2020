{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods\n",
    "\n",
    "**Overview**:\n",
    "In this practical we will train an agent using the Actor-Critic algorithm to learn to balance a pole in the OpenAI gym [Cartpole environment](https://gym.openai.com/envs/CartPole-v1).\n",
    "\n",
    "**Learning objectives**:\n",
    "* Understand the Actor-Critic approach to directly training a parameterised policy and state-value function to maximise expected future rewards.\n",
    "\n",
    "**What is expected of you**:\n",
    " * Go through the explanation, keeping the above learning objectives in mind.\n",
    " * Fill in the missing code (\"#IMPLEMENT-ME\") and train a model to solve the Cartpole-v1 environment in OpenAI gym (you solve it when reward=500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "![progression](https://miro.medium.com/max/2052/1*T1zTYVLkMNngE09fOqTkSA.png)\n",
    "\n",
    "Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor‚Äìcritic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating the value estimate for a state from the estimated values of subsequent states), but only as a baseline for the state whose estimate is being updated. \n",
    "\n",
    "This is a useful distinction, for only through bootstrapping do we introduce bias and an asymptotic dependence on the quality of the function approximation. As we have seen, the bias introduced through bootstrapping and reliance on the state representation is often beneficial because it reduces variance and accelerates learning. \n",
    "\n",
    "REINFORCE with baseline is unbiased and will converge asymptotically to a local minimum, but like all Monte Carlo methods it tends to learn slowly (produce estimates of high variance) and to be inconvenient to implement online or for continuing problems. Temporal-difference methods we can eliminate these inconveniences, and through multi-step methods we can flexibly choose the degree of bootstrapping. In order to gain these advantages in the case of policy gradient methods we use actor‚Äìcritic methods with a bootstrapping critic.\n",
    "\n",
    "![Actor-Critic Model](http://incompleteideas.net/book/first/ebook/figtmp34.png \"Actor-Critic Model\")\n",
    "![TD Error](https://keen-agnesi-5aa4df.netlify.com/reinforcement_learning/lec/5%20-%20MF%20RL.assets/7f91b1262e3b5ee58eb49755a173ecfb.png) \n",
    "\n",
    "\n",
    "**NOTE**: \n",
    "\n",
    "Combine ideas from policy and value function methods:\n",
    "* Actor improvement - Policy $\\pi$ parameterised by $\\theta$ \n",
    "* Critic evaluation - State Value function $V(s; \\omega)$ or the State-Action Value Function $Q(s, a; \\omega)$ function parameterised by $\\omega$  \n",
    "\n",
    "***\n",
    "    \n",
    "**Actor-Critic pseudocode**:\n",
    "\n",
    "Input: parameterised forms for $\\pi_{\\theta}(s|a)$ and $V_{\\omega}(s)$\n",
    "<br>\n",
    "Input: learning rates $\\alpha_{\\omega} > 0$ and $\\alpha_{\\theta} > 0$\n",
    "\n",
    "For each episode:<br>\n",
    "$\\quad$ Initialise $s$<br>\n",
    "$\\quad$ For each time step:<br>\n",
    "$\\quad \\quad$Choose $a \\sim \\pi_{\\theta}(s|a)$<br>\n",
    "$\\quad \\quad$Take $a$, observe $s‚Ä≤$, $ùëü$<br>\n",
    "$\\quad \\quad \\delta \\leftarrow r + \\gamma V_{\\omega}(s‚Ä≤) - V_{\\omega}(s)$    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (TD Error - baseline)<br> \n",
    "$\\quad \\quad \\omega \\leftarrow \\omega + \\alpha_{\\omega} \\delta \\nabla_{\\omega} V_{\\omega}(ùë†) $    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (critic update parameters)<br>\n",
    "$\\quad \\quad \\theta \\leftarrow \\theta + \\alpha_{\\theta} \\delta \\nabla_{\\theta} \\log\\pi_\\theta(a|s)$   &nbsp;&nbsp;&nbsp;(actor update parameters)<br>\n",
    "$\\quad \\quad s \\leftarrow s'$<br>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) # set random seed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical \n",
    "\n",
    "# for auto-reloading external modules\n",
    "# (if you're curious, see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)\n",
    "\n",
    "# configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple environments\n",
    "\n",
    "We will create multiple environments and collect experence from each, before updating the networks. This approach has many advantages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 8\n",
    "\n",
    "def make_env(env_name, seed, rank):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        env.seed(seed+rank)\n",
    "        return env\n",
    "\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment - Cart Pole\n",
    "\n",
    "Cartpole is a standard benchmark in reinforcement learning and is a good sandbox for trying things out. The goal is to balance a pendulum on top of a moving cart. We have 2 actions - either push the cart to the left or push to the right. The state space consists of the cart's position and velocity and the pendulum's angle and angular velocity. For more details refer to the OpenAi/gym github wiki page ([link](https://github.com/openai/gym/wiki/CartPole-v0)) \n",
    "\n",
    "Let's create the environment and take a look at the state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print('Environment:', 'CartPole-v1')\n",
    "print('\\t','action space:', env.action_space)\n",
    "print('\\t','observation space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watching a random policy agent play\n",
    "\n",
    "Let's also see how a random policy performs in this enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_1 = gym.make('CartPole-v1')\n",
    "# state = env_1.reset()\n",
    "# for t in range(200):\n",
    "#     # sample a random action\n",
    "#     action = env_1.action_space.sample()\n",
    "#     env_1.render()\n",
    "#     state, reward, done, _ = env_1.step(action)\n",
    "# env_1.close()\n",
    "# del env_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good! ... Now let's improve things using Actor-Critic.\n",
    "\n",
    "## The Actor-Critic Networks\n",
    "\n",
    "Simple enough. Refer to [torch.nn](https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size_1, hidden_size_2, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # We will be using the nn.Sequential to create Actor and Critic networks. \n",
    "        # For more information please refer to the documentation - https://pytorch.org/docs/stable/nn.html?highlight=sequential#torch.nn.Sequential\n",
    "        \n",
    "        \n",
    "        # Define neural network for the Critic\n",
    "        self.critic = nn.Sequential(\n",
    "            # first layer (linear) should have an input size of 'num_inputs' and an output size of 'hidden_size'\n",
    "            nn.Linear(num_inputs, hidden_size_1),\n",
    "            # apply relu activation \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            # last layer (linear) should have an input size of 'hidden_size' and output the value of the state (single output)\n",
    "            nn.Linear(hidden_size_2, 1)\n",
    "        )\n",
    "        \n",
    "        # Define neural network for the Actor\n",
    "        self.actor = nn.Sequential(\n",
    "            # first layer (linear) should have an input size of 'num_inputs' and an output size of 'hidden_size'\n",
    "            nn.Linear(num_inputs, hidden_size_1),\n",
    "            # apply relu activation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size_1, hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            # next linear layer should have an input size of 'hidden_size' and output size of 'num_outputs'\n",
    "            nn.Linear(hidden_size_2, num_outputs),\n",
    "            # apply softmax to pervious layer, to return the probability of each action.\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # get the value from the Critic network\n",
    "        value = self.critic(x)\n",
    "        # get the action probs from the Actor network\n",
    "        probs = self.actor(x)\n",
    "        dist  = Categorical(probs)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_env(model, vis=False):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Steps # (x1000)')\n",
    "    plt.title('Steps %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting actions with our Actor network\n",
    "\n",
    "For a given state our networks returns a pytorch `Categorial` object along with the `value` object. We can sample from this distribution by calling it's `sample` method and we can find the log probability of an action using `log_prob`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actorCritic = ActorCritic(env.observation_space.shape[0], env.action_space.n, 64, 32).to(device)\n",
    "state = env.reset()\n",
    "state = torch.from_numpy(state).float().to(device)\n",
    "dist, value = actorCritic(state)\n",
    "action = dist.sample()\n",
    "print(\"Sampled action:\", action.item())\n",
    "print(\"Log probability of action:\", dist.log_prob(action).item())\n",
    "print(\"Value:\", value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the return\n",
    "Calculate the bootstrapped return $\\sum^{n}_{t=0} \\gamma^t r(s_t, a_t) +  \\gamma^{n+1} V_{\\omega}(s_{n+1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma):\n",
    "    # Compute the return and dont forget the value of the next state\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic\n",
    "\n",
    "Now its time to implement the algorithm\n",
    "\n",
    "**Actor-Critic pseudocode**:\n",
    "\n",
    "Input: parameterised forms for $\\pi_{\\theta}(s|a)$ and $V_{\\omega}(s)$\n",
    "<br>\n",
    "Input: learning rates $\\alpha_{\\omega} > 0$ and $\\alpha_{\\theta} > 0$\n",
    "\n",
    "\n",
    "Initialise $s$<br>\n",
    "For each time step:<br>\n",
    "$ \\quad$Choose $a \\sim \\pi_{\\theta}(s|a)$<br>\n",
    "$ \\quad$Take $a$, observe $s‚Ä≤$, $ùëü$<br>\n",
    "$ \\quad \\delta \\leftarrow r + \\gamma V_{\\omega}(s‚Ä≤) - V_{\\omega}(s)$<br>\n",
    "$ \\quad \\omega \\leftarrow \\omega + \\alpha_{\\omega} \\delta \\nabla_{\\omega} V_{\\omega}(ùë†)$<br>\n",
    "$ \\quad \\theta \\leftarrow \\theta + \\alpha_{\\theta} \\delta \\nabla_{\\theta} \\log\\pi_\\theta(a|s)$<br>\n",
    "$ \\quad s \\leftarrow s'$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env_name, hyperameters, seed, verbose=True):\n",
    "    \n",
    "    # set random seeds (for reproducibility)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    envs = [make_env(env_name, seed, i) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(envs)\n",
    "    \n",
    "    # instantiate the policy and optimiser\n",
    "    num_inputs  = envs.observation_space.shape[0]\n",
    "    num_outputs = envs.action_space.n\n",
    "    model = ActorCritic(num_inputs, num_outputs, hyperameters['hidden_size_1'], hyperameters['hidden_size_2']).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hyperameters['learning_rate'])\n",
    "    \n",
    "    current_step_number = 0\n",
    "    test_rewards = []\n",
    "    state = envs.reset()\n",
    "    \n",
    "    while current_step_number < hyperameters['max_steps']:\n",
    "        \n",
    "        log_probs = []\n",
    "        values    = []\n",
    "        rewards   = []\n",
    "        masks     = []\n",
    "        entropy = 0\n",
    "\n",
    "        for _ in range(hyperameters['num_step_td_update']):\n",
    "            # get the distribution over actions for state and the value of the state\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = model(state)\n",
    "            \n",
    "            # sample an action from the distribution\n",
    "            action = dist.sample()\n",
    "            # take a step in the environment\n",
    "            next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "                \n",
    "            # compute the log probability\n",
    "            log_prob = dist.log_prob(action)\n",
    "            # compute the entropy\n",
    "            entropy += dist.entropy().mean()\n",
    "            \n",
    "            # save the log probability, value and reward \n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "            masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "\n",
    "            state = next_state\n",
    "            current_step_number += 1\n",
    "            \n",
    "            if current_step_number % 1000 == 0:\n",
    "                test_rewards.append(np.mean([test_env(model) for _ in range(10)]))\n",
    "                plot(current_step_number, test_rewards)\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        _, next_value = model(next_state)\n",
    "   \n",
    "        # calculate the discounted return of the episode\n",
    "        returns = compute_returns(next_value, rewards, masks, hyperameters['gamma'])\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns   = torch.cat(returns).detach()\n",
    "        values    = torch.cat(values)\n",
    "\n",
    "        # Compute the advantage\n",
    "        advantage = returns - values\n",
    "        \n",
    "        # Compute the actor's and critic's loss\n",
    "        actor_loss  = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        # Compute the sum the actor and critic loss \n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "        loss -= 0.001 * entropy\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperameters = {\n",
    "    'hidden_size_1': 64,\n",
    "    'hidden_size_2': 32,\n",
    "    'learning_rate':3e-4,\n",
    "    'num_step_td_update': 5,\n",
    "    'max_steps':10000,\n",
    "    'gamma': 0.99\n",
    "}\n",
    "env_name = \"CartPole-v1\"\n",
    "model = actor_critic(env_name, hyperameters, 345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing our learned policy in action\n",
    "\n",
    "Let's watch our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_1 = gym.make('CartPole-v1')\n",
    "# state = env_1.reset()\n",
    "# for t in range(2000):\n",
    "#     dist, _ = model(torch.from_numpy(state).float().to(device))\n",
    "#     action = dist.sample()\n",
    "#     env_1.render()\n",
    "#     state, reward, done, _ = env_1.step(action.item())\n",
    "#     if done:\n",
    "#         break\n",
    "# del env_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try another environment - LunarLander-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "print('Environment:', 'LunarLander-v2')\n",
    "print('\\t','action space:', env.action_space)\n",
    "print('\\t','observation space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different environments can be learned with the same algorithm. Only the hyper-parameters usually have to be tuned for optimal performance. Among the important hyperparameters, one should look at the **learning rate**, **the discount factor** and the **number of epochs**.\n",
    "\n",
    "For instance if the learning is too slow, you can try increasing the learning rate but that might also create additional instabilities in the learning. To increase stability, you can you can try to reduce the training discount factor but if you set it to a too low value, you might not be optimizing the right objective and learning can therefore not be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperameters = {\n",
    "    'hidden_size_1': 128,\n",
    "    'hidden_size_2': 64,\n",
    "    'learning_rate':1e-4,\n",
    "    'num_step_td_update': 5,\n",
    "    'max_steps':15000,\n",
    "    'gamma': 0.9\n",
    "}\n",
    "env_name = 'LunarLander-v2'\n",
    "model_2 = actor_critic(env_name, hyperameters, 111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing our learned policy in action\n",
    "\n",
    "Let's watch our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_1 = gym.make('LunarLander-v2')\n",
    "# state = env_1.reset()\n",
    "# for t in range(2000):\n",
    "#     dist, _ = model_2(torch.from_numpy(state).float().to(device))\n",
    "#     action = dist.sample()\n",
    "#     env_1.render()\n",
    "#     state, reward, done, _ = env_1.step(action.item())\n",
    "#     if done:\n",
    "#         break\n",
    "# del env_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
