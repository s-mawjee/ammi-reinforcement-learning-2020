{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZO_beNp3sWOZ"
   },
   "source": [
    "## Policy Evaluation\n",
    "$$\n",
    "\\def\\E{\\mathbb{E}}\n",
    "\\def\\given{\\mid}\n",
    "\\def\\states{\\mathcal{S}}\n",
    "\\def\\argmax{\\text{argmax}}\n",
    "$$\n",
    "The first step to improving a policy is to evaluate the state-value function $v_\\pi$ for an arbitraty policy (see Sutton & Barto 4.1 for details). The state-value according to a policy is computed as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_\\pi (s) &= \\E_\\pi \\left[ G_t \\given S_t = s\\right]\\\\\n",
    "&= \\E_\\pi \\left[ R_{t+1} + \\gamma G_{t+1} \\given S_t = s\\right]\\\\\n",
    "&= \\E_\\pi \\left[ R_{t+1} + \\gamma v_\\pi ( S_{t+1}) \\mid S_t = s \\right]\\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The value of $v_\\pi$ can be updated iteratively for all $s$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1}(s) &= \\E_{\\pi} \\left[ R_{t+1} + \\gamma v_k (S_{t_1} \\given S_t = s)\\right] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For any policy $\\pi$, your task to to implement a policy evaluation function based on the following pseudocode:\n",
    "\n",
    "\n",
    "![policy-evaluation](https://i.ibb.co/j4zZ9Xw/policy-evaluation.png)\n",
    "\n",
    "(source: S&B Section 4.1, page 75)\n",
    "\n",
    "Recall $p$ represents the transition probabilities, which we will get from a simple GridWorld below, $\\gamma$ is the discount factor, whereas $\\theta$ is a  small threshold which indicates when to stop updating our value function. A simple GridWorld looks something like the following:\n",
    "\n",
    "![gridworld](https://i.ibb.co/Lk166s4/gridworld.png)\n",
    "\n",
    "(source: S&B Section 4.1, page 76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcyiLAxzsWOe"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Complete the `policy_evaluation` function above so that the following evaluates without error:\n",
    "```python\n",
    "v = policy_evaluation(random_policy, env)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1hb2MCusWOh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from environments.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-uVxEF5sWOv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start state is 5 \n",
      "\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv(shape=[4, 4], terminal_states=[0,15], terminal_reward=0, step_reward=-1)\n",
    "state = env.reset()\n",
    "print('Start state is', state,'\\n')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJpUFdu-sWO5"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.observation_space.n is a number of states in the environment. \n",
    "            env.action_space.n is a number of actions in the environment.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.observation_space.n representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nu4YWgt6sWPB"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c2a0ae7e8562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrandom_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-047d23ef5cbf>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(policy, env, discount_factor, theta)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random_policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "v = policy_evaluation(random_policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eahtKZidsWPK"
   },
   "source": [
    "Run the cell below to verify your $v_\\pi$ has converged to the proper value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FVB6jrAzsWPO",
    "outputId": "8f30d62b-f0bd-48d6-af89-5af6b9428ef0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a4cebab851c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexpected_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Your algorithm was successfully implemented\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "result = np.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "if result is None:\n",
    "    print(\"Your algorithm was successfully implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lR3Erk1GsWPX"
   },
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Now that we can evaluate the state value function for any given policy, we can use the values to improve our policy. \n",
    "\n",
    "First consider the state-action value function (q-function), which consists of selecting $a$ in $s$ and then behaving according to $\\pi$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_\\pi(s,a) &= \\E \\left[ R_{t+1} + \\gamma v_\\pi (S_{s+1}) \\given S_t = s, A_t = a \\right] \\\\\n",
    "&= \\sum_{s^\\prime, r} p(s^\\prime, r \\given, s, a) \\left[ r + \\gamma v_\\pi (s^\\prime) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let $\\pi^\\prime$ be a policy such that, for all $s \\in \\states$:\n",
    "$$\n",
    "q(s, \\pi^\\prime (s)) \\geq v_\\pi (s)\n",
    "$$\n",
    "\n",
    "Then the following holds for all $s \\in \\states$:\n",
    "$$\n",
    "v_\\pi^\\prime (s) \\geq v_\\pi (s)\n",
    "$$\n",
    "(See S&B page 78 for proof)\n",
    "\n",
    "Consider a simple policy improvement which consists of $\\pi$ selecting an action according to the maximum state-action value:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi^\\prime (s) &= \\argmax_{a} q_\\pi (s,a) \\\\\n",
    "&= \\argmax_a \\E \\left[ R_{t+1} + \\gamma v_\\pi (S_{t+1}) \\given S_t = s, A_t = a \\right] \\\\\n",
    "&= \\argmax_a \\sum_{s^\\prime, r} p(s^\\prime, s \\given s, a) [r + \\gamma v_\\pi (s^\\prime)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Alternating between policy evaluation and policy improvement is known as **policy iteration**. Implement the algorithm based on the following pseudocode:\n",
    "\n",
    "![gridworld.png](https://i.ibb.co/GHpV8kV/policy-iteration.png)\n",
    "\n",
    "(source: S&B page 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1B_VsImsWPZ"
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv(shape=[4, 4], terminal_states=[0,15], terminal_reward=0, step_reward=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGyLfXKOsWPl"
   },
   "source": [
    "### Exercise 2\n",
    "Complete the `policy_improvement` function, based on the above pseudocode, so that the following runs without error:\n",
    "```\n",
    "policy, v = policy_improvement(env)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_JuXGYh_sWPq"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_evaluation, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.observation_space.n\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.action_space.n containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.action_space.n)\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        return A\n",
    "      \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = ...\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.observation_space.n):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = ...\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = ...\n",
    "            best_a = ...\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.action_space.n)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFFqHVzxsWP2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eAD93o78sWQB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your algorithm was successfully implemented\n"
     ]
    }
   ],
   "source": [
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "result = np.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "if result is None:\n",
    "    print(\"Your algorithm was successfully implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0rpOroesWQJ"
   },
   "source": [
    "## Value Iteration\n",
    "The isue with policy iteration is that every time we improve our policy, we must do a full sweep through all states again to evaluate the new policy! This is extremely inefficient. Value iteration combines both evaluation and improvement in the same step. Implement value iteration based on the following pseudocode:\n",
    "\n",
    "\n",
    "![gridworld.png](https://i.ibb.co/SVTmBFQ/value-iteration.png)\n",
    "\n",
    "(source: S&B Section 4.4, page 83)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LtMKX6-QsWQL"
   },
   "source": [
    "### Exercise 3\n",
    "Complete the `value_iteration` function below, based on the above pseudocode, so that the following evaluates without error\n",
    "```\n",
    "policy, v = value_iteration(env)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5yBKZKOsWQN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.observation_space.n is a number of states in the environment. \n",
    "            env.action_space.n is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.observation_space.n\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.action_space.n containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.action_space.n)\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        return A\n",
    "\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.observation_space.n):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = ...\n",
    "            best_action_value = ...\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = ...\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = ...   \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = ...\n",
    "        best_action = ...\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fapSyrHAsWQW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LXCsMRo-sWQf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your algorithm was successfully implemented\n"
     ]
    }
   ],
   "source": [
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "result = np.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "if result is None:\n",
    "    print(\"Your algorithm was successfully implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "buewi_IUsWQm"
   },
   "source": [
    "## Exercise 4\n",
    "Now that you have implemented Policy Iteration and Value Iteration, plot the average running time for both algorithms by varying the discount rate $\\gamma$ between 0 and 1. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Dynamic_programming_2020_Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
